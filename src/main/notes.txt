representation :

states : - n_rows * n_cols * (m+1)_stacks
		- m/2 stacks for last m/2 moves of player0 (need to do that because the states doesn't have markov property)
		- m/2 stacks for last m/2 moves of player1
		- 1 stack for the current player (0 or 1)
		- (might add some features later such as pawn type etc...)
actions : valid moves from ludii lib
rewards : -1 for losing, 0 for draw, 1 for winning

============================================================================

algorithms :

- value network 
	- input : states
	- output : value between -1 (bad state for current player) and 1 (good state)
	- training : simulate games until win following a policy (init random), when game is over, label the states of the winner by a reward of 1 and the state of the loser by a reward of -1. Then we shuffle all the states and train a neural network in order to estimate the value.
	- settings : MSE + sigmoid
	- backbone : ResNet CNN

- policy network
	- input : states
	- output : policy vector (distribution on moves), need to mask out the unvalid moves + renormalize afterwards.
	- training : simulate games using MCTS and the estimated policy, it gives us a policy which will be used to build our labels. Then we shuffle the states and train a neural network in order to estimate the new policy. The new policy is now used during the MCTS, thus becoming a circular dependency.
	- settings : CCE + softmax
	- backbone : ResNet CNN

- monte carlo tree search
	- input : init state of the game, current player and number of simulations
	- external values used : policy from the policy network & values from the value network & UCB score which depends on the prior probabilty of the states, the value of the state and the number of time we visited that state. 
	- output : the root of the tree (a new policy)
	- working : simulate the board and search in the tree of valid moves. Build the tree The best move is the one which is the most visited.
	
- UCB (Upper Confidence Bound)
	- MCTS alone struggle when there is a lot of possible moves
	- UCB helps MCTS chose better moves while simulating games
	- balance exploration and exploitation
	- v = s/n + C * sqrt(ln(N)/n); with v as the value, s the aggregate score of the move, n the number of visit, N the number of games simulated, C a exploration constant
	
- AlphaZero
	- start with a random policy
	- both players play with MCTS + UCB + random policy
	- at each move we store
		- the game state
		- the MCTS policy
		- future reward (-1,0,+1)
	- after getting our training set
		- optimise neural network
			- input : game state
			- output : MCTS policy & future reward
		- after 1000 training loops, evaluate the network
			- play against the latest network 400 games and check if it wins > 55% 
	- after several self-play games we train both neural networks
	- then we start another game but with the policy estimated by the policy network & the values estimated by the value network
	- repeat until convergence

============================================================================

architecture :

- dojo
	- train
		- model.py
		- MCTS.py
		- train.py
	- tmp_models
	- final_models	
	
- inference
	- ludii python file
	- ludii java file
	- ludii jar
	-...
	
============================================================================

TODO :

Figure out how to train : 
	- java : init random policy
	- java : MCTS running on the Ludii game and building a dataset (starting from random policy)
	- java : build csv file dataset
	- python : train NN on dataset & quantitative evaluation
	- python : create AI agent using the NN policy
	- java : use the python AI agent in trials for qualitative evaluation
	- java : if the python AI agent performs better, use its policy on MCTS and build a new dataset
	- repeat ...
	
	- python : init random policy
	- python : create AI MCTS agent
	- java : run and observe MCTS agent
	- java : build csv file dataset
	- python : train NN on dataset & quantitative evaluation
	- python : create AI agent using the NN policy
	- java : use the python AI agent in trials for qualitative evaluation
	- java : if the python AI agent performs better, use its policy on MCTS and build a new dataset
	- repeat ...
	
PYTHON-SIDE :
	- get game lib infos
		- state : build state
		- reward : future reward function
		- action : valid moves function
	- code MCTS
		- node class
		- MCTS class
	- code data acquisition
		- dataframe containing
			- game state
			- MCTS search prob
			- future rewards
	- code tensorflow two-headed CNN
		- architecture
		- custom loss
	- code main dojo
		- MCTS > network train > evaluate model
		
============================================================================

Ludii notes :
	
- 
- initialisation and cleanup can be usefull before running the AI agent (for instance if we need to load weights etc...)
- there is a function to create a pass moves if needed
- state.owned() & state.mover() for state representation

























