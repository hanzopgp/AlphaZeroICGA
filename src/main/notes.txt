- test sending multiple self play on multiple nodes CPU clusters, and train model on GPU clusters
- re-add policy for connect-four
- optimize parameters, fight between models 
- delete vanilla dataset?	

============================================================================

- focus on tic tac toe 
- maybe reduce model complexity
- check everything

============================================================================

- bashni : OK
- ploy : how to find orientation of the different types of pawns in the representation
- quoridor : how to find the type of pawn (wall or pawn ?)

- pass the right function instead of if else in format_state
- representation is just depending levels and 2D(site)
- focus on train ?

============================================================================

- cold start with 50k example from MCTS vanilla
- train the model and delete the dataset
- self play with that model

============================================================================

- we have an object [location, location] with location(site, type)
- for loc in locations:
	arr[][] = location.site(), location.lvl()


0.73 200//3, 0.78 200//6, 0.64 200//20

============================================================================

- try dropout
- try npy or hdf5 for dataset storage
- try leaky relu
- dojos should be in thinking time because the model predict is faster than playout

============================================================================

launch : screen
quit : control+a d
comeback : screen -r

============================================================================

- leela zero 32 filters for heads --> not converging anymore ?
- give up policy prediction and only learn values ? (why not 2 head for value and opp value, why would we predict it twice with inverted state?)
- maybe need to run trials again when dojo'ing vs vanilla mcts 
- maybe we can predict values until it converge, then start training policy and check if converge
- value is usefull but we might lose too much time expanding useless nodes because of random policy
- max number of moves per games to ensure quality data
- test more mcts simu when vanilla mcts generating data

============================================================================

can try : 

1)
one model with 3 outputs : policy, value, value opp
in mcts, predict the 3 outputs for all unexpanded moves in batch

2)
keep model with 2 outputs
in mcts, predict the 2 outputs for all unexpanded moves in batch, and compute the value_opp by switching the sign ?

============================================================================

download tensorrt : https://developer.nvidia.com/nvidia-tensorrt-8x-download

wget https://developer.nvidia.com/compute/machine-learning/tensorrt/secure/8.2.5.1/tars/tensorrt-8.2.5.1.linux.x86_64-gnu.cuda-11.4.cudnn8.2.tar.gz
tar -xf tensorrt-8.2.5.1.linux.x86_64-gnu.cuda-11.4.cudnn8.2.tar.gz
rm tensorrt-8.2.5.1.linux.x86_64-gnu.cuda-11.4.cudnn8.2.tar.gz

onnxruntime :  git clone https://github.com/microsoft/onnxruntime
cd onnxruntime

./build.sh --parallel --use_cuda --cuda_version=11.2 --cuda_home=/usr/lib/cuda --cudnn_home=/usr/lib/cuda --use_tensorrt --tensorrt_home=/home/durande/Bureau/tensorrt/TensorRT-8.2.5.1 --build_shared_lib --enable_pybind --build_wheel --skip_test --cmake_path /usr/bin/cmake

============================================================================

- depassement de quota
- optimization
- optimization undo()
- parrellelisation                        oarsub -l /nodes=1/core=12,walltime=100:00:00 python3 -m moral.moral_train
- MCTS question pour les values
- MCTS questioon PUCT/UCB

============================================================================

analyze : https://python.plainenglish.io/python-profiling-7456d6268e98

Optimization :
files -> mcts_uct.py
	  -> utils.py

utils.py most important function to optimize :
- chose_move()
- format_state()

the most long part is the mcts_trial, but since it uses a fix model, we can launch it on many computers at once and merge the datasets produced.
can do the same with the mcts_dojo part.
the train_model one is already fast if we activated the GPU

============================================================================

Run on clusters :

scp -r AlphaZero/ durande@cluster.lip6.fr:.
ssh durande@cluster.lip6.fr
oarsub -I
scp -r durande@cluster.lip6.fr:. .
cd AlphaZero/src/main
nano src_python/config.py
javac -cp "libs/jpy-0.10.0-SNAPSHOT.jar":"libs/Ludii-1.3.4.jar" src_java/alphazero/RunningTrialsWithPython.java -d bin/
pip install tensorflow
cd bin | java -cp "../libs/jpy-0.10.0-SNAPSHOT.jar":"../libs/Ludii-1.3.4.jar":"alphazero/":"." alphazero.RunningTrialsWithPython

============================================================================

representation :

states : - n_rows * n_cols * (m+1)_stacks
		- m/2 stacks for last m/2 moves of player0 (need to do that because the states doesn't have markov property)
		- m/2 stacks for last m/2 moves of player1
		- 1 stack for the current player (0 or 1)
		- (might add some features later such as pawn type etc...)
actions : valid moves from ludii lib
rewards : -1 for losing, 0 for draw, 1 for winning

============================================================================

algorithms :

- value network 
	- input : states
	- output : value between -1 (bad state for current player) and 1 (good state)
	- training : simulate games until win following a policy (init random), when game is over, label the states of the winner by a reward of 1 and the state of the loser by a reward of -1. Then we shuffle all the states and train a neural network in order to estimate the value.
	- settings : MSE + sigmoid
	- backbone : ResNet CNN

- policy network
	- input : states
	- output : policy vector (distribution on moves), need to mask out the unvalid moves + renormalize afterwards.
	- training : simulate games using MCTS and the estimated policy, it gives us a policy which will be used to build our labels. Then we shuffle the states and train a neural network in order to estimate the new policy. The new policy is now used during the MCTS, thus becoming a circular dependency.
	- settings : CCE + softmax
	- backbone : ResNet CNN

- monte carlo tree search
	- input : init state of the game, current player and number of simulations
	- external values used : policy from the policy network & values from the value network & UCB score which depends on the prior probabilty of the states, the value of the state and the number of time we visited that state. 
	- output : the root of the tree (a new policy)
	- working : simulate the board and search in the tree of valid moves. Build the tree The best move is the one which is the most visited.
	
- UCB (Upper Confidence Bound)
	- MCTS alone struggle when there is a lot of possible moves
	- UCB helps MCTS chose better moves while simulating games
	- balance exploration and exploitation
	- v = s/n + C * sqrt(ln(N)/n); with v as the value, s the aggregate score of the move, n the number of visit, N the number of games simulated, C a exploration constant
	
- AlphaZero
	- start with a random policy
	- both players play with MCTS + UCB + random policy
	- at each move we store
		- the game state
		- the MCTS policy
		- future reward (-1,0,+1)
	- after getting our training set
		- optimise neural network
			- input : game state
			- output : MCTS policy & future reward
		- after 1000 training loops, evaluate the network
			- play against the latest network 400 games and check if it wins > 55% 
	- after several self-play games we train both neural networks
	- then we start another game but with the policy estimated by the policy network & the values estimated by the value network
	- repeat until convergence

============================================================================

architecture :

- dojo
	- train
		- model.py
		- MCTS.py
		- train.py
	- tmp_models
	- final_models	
	
- inference
	- ludii python file
	- ludii java file
	- ludii jar
	-...
	
============================================================================

TODO :

Figure out how to train : 
	- java : init random policy
	- java : MCTS running on the Ludii game and building a dataset (starting from random policy)
	- java : build csv file dataset
	- python : train NN on dataset & quantitative evaluation
	- python : create AI agent using the NN policy
	- java : use the python AI agent in trials for qualitative evaluation
	- java : if the python AI agent performs better, use its policy on MCTS and build a new dataset
	- repeat ...
	
	- python : init random policy
	- python : create AI MCTS agent
	- java : run and observe MCTS agent
	- java : build csv file dataset
	- python : train NN on dataset & quantitative evaluation
	- python : create AI agent using the NN policy
	- java : use the python AI agent in trials for qualitative evaluation
	- java : if the python AI agent performs better, use its policy on MCTS and build a new dataset
	- repeat ...
	
PYTHON-SIDE :
	- get game lib infos
		- state : build state
		- reward : future reward function
		- action : valid moves function
	- code MCTS
		- node class
		- MCTS class
	- code data acquisition
		- dataframe containing
			- game state
			- MCTS search prob
			- future rewards
	- code tensorflow two-headed CNN
		- architecture
		- custom loss
	- code main dojo
		- MCTS > network train > evaluate model
		
============================================================================

Ludii notes :
	
- 
- initialisation and cleanup can be usefull before running the AI agent (for instance if we need to load weights etc...)
- there is a function to create a pass moves if needed
- state.owned() & state.mover() for state representation












































